{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes\")\n",
    "from config.sheldefaults import *\n",
    "# config.basedeafults is copied from \"/work/PCDC/s202005/Research/PARP/Codes/PARpforLRPT/config/defaults\"\n",
    "from config.basedefaults import *\n",
    "from general.commonfuns import * \n",
    "from general.imports import *\n",
    "from general.logs import * \n",
    "from data.data_legos import * \n",
    "from models.LRPT.LRPT import *\n",
    "os.chdir(\"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT\")\n",
    "from model.PARPModel import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShelScratch(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _get_base_config(self,base_config_index,base_yaml_index,base_config_folder):\n",
    "        config_folder_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])\n",
    "        config_subfolder = os.path.join(base_config_folder,config_folder_dic[base_config_index])\n",
    "        yaml_folder = sorted([ele for ele in os.listdir(config_subfolder) if ele.endswith(\".yaml\")])\n",
    "\n",
    "        base_yaml_path = os.path.join(config_subfolder,yaml_folder[base_yaml_index])\n",
    "        config = get_cfg_defaults()\n",
    "        config.merge_from_file(base_yaml_path)\n",
    "        \n",
    "        ## return \n",
    "        self.base_config = config\n",
    "        self.base_yaml_path = base_yaml_path\n",
    "        self.parp_folder = config.SYSTEM.SAVEFOLDER.split(\"Single_1_Anti_2_Dense_1024/\")[-1]\n",
    "\n",
    "    def _get_shel_config(self,config_file_path):\n",
    "        config = get_shelburne_defaults()\n",
    "        config.merge_from_file(config_file_path)\n",
    "        batch = config.ScratchTrain.BATCH_SIZE\n",
    "        epochs = config.ScratchTrain.EPOCHS\n",
    "        tempname = \"Epochs_\"+str(epochs)+\"_Batch_\"+str(batch)\n",
    "        ## return\n",
    "        self.shel_config = config\n",
    "        self.save_folder = os.path.join(config.SYSTEM.SAVEFOLDER,self.parp_folder,tempname)\n",
    "        os.makedirs(self.save_folder,exist_ok=True)\n",
    "        \n",
    "    def _get_cv_and_data(self):\n",
    "        shel_config = self.shel_config\n",
    "        ######## get data \n",
    "        Variants,Antibiotics,Results = open_pickle(shel_config.SHELDATA.ThreeSets)\n",
    "        DataInfo = pd.read_csv(shel_config.SHELDATA.Info)\n",
    "        Shel_X = pd.concat([Variants,Antibiotics],axis=1)\n",
    "        shel_data = {\"Variants\":Variants,\n",
    "                     \"Antibiotics\":Antibiotics,\n",
    "                     \"Results\":Results,\n",
    "                     \"X\":Shel_X,\n",
    "                     \"Info\":DataInfo}\n",
    "\n",
    "        ######### get cv \n",
    "        Totaly = Results\n",
    "        cv_split = KFold(n_splits=shel_config.LRPTTrain.CVSPLIT,\n",
    "                         shuffle=shel_config.LRPTTrain.CVSHUFFLE,\n",
    "                         random_state=shel_config.LRPTTrain.CVSEED)\n",
    "        for cv_index,(train_ix,test_ix) in enumerate(cv_split.split(Totaly)):\n",
    "            cv_save_folder = os.path.join(self.save_folder,\"cv_\"+str(cv_index))\n",
    "            os.makedirs(cv_save_folder,exist_ok=True) # make the dircitiony \n",
    "            split_index = {\"train_ix\":train_ix,\"test_ix\":test_ix}\n",
    "            save_as_pickle(split_index,os.path.join(cv_save_folder,\"train_test_ix.pkl\"))\n",
    "            \n",
    "        ## return \n",
    "        self.shel_data = shel_data\n",
    "    \n",
    "    def _get_base_model_path(self,parp_folder):\n",
    "        \n",
    "        epochsname  = os.path.split(os.path.split(self.base_yaml_path)[0])[1]\n",
    "        seedname = os.path.split(self.base_yaml_path)[1].replace(\".yaml\",\"\")\n",
    "        trained_model_weights_path = os.path.join(parp_folder,epochsname,seedname,\"modelweights.h5\")\n",
    "        ## return \n",
    "        self.trained_model_weights_path = trained_model_weights_path\n",
    "        \n",
    "    def _wrap_PARP_data(self):\n",
    "        # get train/test/or validation for training process\n",
    "        Variants,Antibiotics,Results = open_pickle(self.base_config.PARPData.TrainPath)\n",
    "        PARP_Train_X = pd.concat([Variants,Antibiotics],1)\n",
    "        \n",
    "        # return \n",
    "        self.PARP_Train_X = PARP_Train_X.iloc[:5,:]  \n",
    "        \n",
    "        opts = [\"PARP.VARIANTS_INPUT_DIM\", Variants.shape[1],\n",
    "                \"PARP.ANTI_INPUT_DIM\", Antibiotics.shape[1]]\n",
    "        self.base_config.merge_from_list(opts)\n",
    "        \n",
    "    def _get_base_model_PARP(self,config,PARP=True):\n",
    "        model = build_parp_model(config)\n",
    "        # ## step 4: model compile \n",
    "        loss_key = config.PARPTrain.LOSS\n",
    "        loss = [ele for ele in config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "        optimizer_key = config.PARPTrain.OPTIMIZER\n",
    "        optimizer_rate = config.PARPTrain.OPTIMIZERATE\n",
    "        optimizer = [ele for ele in config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "        metrics_threshold_list = config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "        metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,\n",
    "                                                   name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        backend.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "        \n",
    "        if PARP:\n",
    "            model.load_weights(self.trained_model_weights_path)\n",
    "            self.PARP_base_model = model\n",
    "            return \n",
    "        else:\n",
    "            return model\n",
    "        \n",
    "    def _get_base_model_Scratch(self):\n",
    "        Scratch_base_config = copy.deepcopy(self.base_config)\n",
    "        opts = [\"PARP.VARIANTS_INPUT_DIM\", self.shel_data[\"Variants\"].shape[1],\n",
    "                \"PARP.ANTI_INPUT_DIM\", self.shel_data[\"Antibiotics\"].shape[1]]\n",
    "        Scratch_base_config.merge_from_list(opts)\n",
    "        Scratch_base_model = self._get_base_model_PARP(config=Scratch_base_config,PARP=False)\n",
    "        self.Scratch_base_model = Scratch_base_model\n",
    "        self.Scratch_base_config = Scratch_base_config\n",
    "    \n",
    "    \n",
    "    def eval_on_basePARP(self,cv_folder):\n",
    "        \n",
    "        assert cv_folder in os.listdir(self.save_folder),\"Cv folder doesn't exist\"\n",
    "        cv_ix = open_pickle(os.path.join(self.save_folder,cv_folder,\"train_test_ix.pkl\"))\n",
    "\n",
    "        ## get save folder\n",
    "        savefolder = os.path.join(self.save_folder,cv_folder)\n",
    "        train_ix = cv_ix[\"train_ix\"]\n",
    "        test_ix = cv_ix[\"test_ix\"]\n",
    "\n",
    "        ## get \n",
    "        X = self.shel_data[\"X\"]\n",
    "        X = Modify_Variants_Mute(X,self.PARP_Train_X)\n",
    "\n",
    "        Y = self.shel_data[\"Results\"]\n",
    "\n",
    "        ## get train and test \n",
    "        train_x = X.to_numpy()[train_ix]\n",
    "        test_x = X.to_numpy()[test_ix]\n",
    "\n",
    "        train_y = Y.to_numpy()[train_ix]\n",
    "        test_y = Y.to_numpy()[test_ix]\n",
    "\n",
    "        ## evaluate \n",
    "        train_score = self.PARP_base_model.evaluate(train_x,train_y,verbose=0)\n",
    "        test_score = self.PARP_base_model.evaluate(test_x,test_y,verbose=0)\n",
    "\n",
    "        train_pred = self.PARP_base_model.predict(train_x,verbose=0)\n",
    "        test_pred =self.PARP_base_model.predict(test_x,verbose=0)\n",
    "\n",
    "        score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "        pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "\n",
    "        ## save \n",
    "        save_as_pickle(data=score,path = os.path.join(savefolder,\"basePARP_score.pkl\"))\n",
    "        save_as_pickle(data=pred,path = os.path.join(savefolder,\"basePARP_pred.pkl\"))\n",
    "    \n",
    "        \n",
    "    def train_and_eval_Scratch(self,cv_folder):\n",
    "        assert cv_folder in os.listdir(self.save_folder),\"Cv folder doesn't exist\"\n",
    "        cv_ix = open_pickle(os.path.join(self.save_folder,cv_folder,\"train_test_ix.pkl\"))\n",
    "\n",
    "        ## get save folder\n",
    "        savefolder = os.path.join(self.save_folder,cv_folder)\n",
    "\n",
    "        train_x = self.shel_data[\"X\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "        train_y = self.shel_data[\"Results\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "\n",
    "        test_x = self.shel_data[\"X\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "        test_y = self.shel_data[\"Results\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "\n",
    "\n",
    "        log_dir = os.path.join(savefolder,\"tensorboard/logs\")\n",
    "\n",
    "        if self.shel_config.ScratchTrain.VALSPLIT is not None:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             patience=self.shel_config.ScratchTrain.EarlyStoppingpatience, \n",
    "                                             min_delta=self.shel_config.ScratchTrain.EarlyStoppingdelta),\n",
    "                         tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "            indices = range(len(train_x))\n",
    "            train_x, val_x, train_y, val_y,train_idx,val_idx = train_test_split(train_x,train_y,indices,\n",
    "                                                                                test_size=self.shel_config.ScratchTrain.VALSIZE,\n",
    "                                                                                shuffle=self.shel_config.ScratchTrain.VALSPLITSHUFFLE,\n",
    "                                                                                random_state=self.shel_config.ScratchTrain.VALSEED)\n",
    "\n",
    "            history = self.Scratch_base_model.fit(x=train_x,y=train_y,\n",
    "                                          batch_size=self.shel_config.ScratchTrain.BATCH_SIZE,\n",
    "                                          epochs=self.shel_config.ScratchTrain.EPOCHS,\n",
    "                                          verbose=self.shel_config.ScratchTrain.VERBOSE,\n",
    "                                          validation_data=(val_x,val_y),\n",
    "                                          callbacks=callbacks)\n",
    "        else:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                     patience=self.shel_config.ScratchTrain.EarlyStoppingpatience, \n",
    "                                     min_delta=self.shel_config.ScratchTrain.EarlyStoppingdelta),\n",
    "                         tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "            history = self.Scratch_base_model.fit(x=train_x,y=train_y,\n",
    "                                          batch_size=self.shel_config.ScratchTrain.BATCH_SIZE,\n",
    "                                          epochs=self.shel_config.ScratchTrain.EPOCHS,\n",
    "                                          verbose=self.shel_config.ScratchTrain.VERBOSE,\n",
    "                                          callbacks = callbacks)\n",
    "        ## save history \n",
    "        temp_path = os.path.join(savefolder,self.shel_config.ScratchTrain.MODEL_HISTORY_PATH)\n",
    "        save_as_pickle(data =history.history,path = temp_path)\n",
    "\n",
    "        ## save model \n",
    "        h5_file = os.path.join(savefolder,self.shel_config.ScratchTrain.MODEL_WEIGHTS_PATH)\n",
    "        self.Scratch_base_model.save_weights(h5_file)\n",
    "\n",
    "        ## save train and val idx \n",
    "        if train_idx:\n",
    "            split_index = {\"train_ix\":train_idx,\"val_ix\":val_idx}\n",
    "            save_as_pickle(split_index,os.path.join(savefolder,\"fit_train_val_ix.pkl\"))\n",
    "\n",
    "        ## evaluate and predict \n",
    "        train_score = self.Scratch_base_model.evaluate(train_x,train_y,verbose=0)\n",
    "        train_pred = self.Scratch_base_model.predict(train_x,verbose=0)\n",
    "\n",
    "        test_score = self.Scratch_base_model.evaluate(test_x,test_y,verbose=0)\n",
    "        test_pred = self.Scratch_base_model.predict(test_x,verbose=0)\n",
    "\n",
    "        score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "        pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "\n",
    "        if train_idx:\n",
    "            val_score = self.Scratch_base_model.evaluate(val_x,val_y,verbose=0)\n",
    "            val_pred = self.Scratch_base_model.predict(val_x,verbose=0)\n",
    "            score[\"val_score\"] = val_score\n",
    "            pred[\"val_pred\"] = val_pred\n",
    "\n",
    "        save_as_pickle(data=score,path = os.path.join(savefolder,\"fit_score.pkl\"))\n",
    "        save_as_pickle(data=pred,path = os.path.join(savefolder,\"fit_pred.pkl\"))\n",
    "\n",
    "        \n",
    "        \n",
    "    def build(self,base_config_index,base_yaml_index,config_file_path):\n",
    "        base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "        parp_folder =\"/work/PCDC/s202005/Research/PARP/Results/PARPforLRPT/Single_1_Anti_2_Dense_1024\" \n",
    "        self._get_base_config(base_config_index=base_config_index,\n",
    "                              base_config_folder=base_config_folder,\n",
    "                              base_yaml_index=base_yaml_index)\n",
    "        self._get_shel_config(config_file_path=config_file_path)\n",
    "        self._get_cv_and_data()\n",
    "        self._get_base_model_path(parp_folder=parp_folder)\n",
    "        self._wrap_PARP_data()\n",
    "        self._get_base_model_PARP(config = self.base_config)\n",
    "        self._get_base_model_Scratch()\n",
    "    def run(self,cv_folder):\n",
    "        self.eval_on_basePARP(cv_folder=cv_folder)\n",
    "        self.train_and_eval_Scratch(cv_folder=cv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_index = 0 \n",
    "base_yaml_index = 0 \n",
    "config_file_path = \"/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/Scratchtemplate.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myScratch = ShelScratch()\n",
    "myScratch.build(base_config_index=base_config_index,base_yaml_index=base_yaml_index,config_file_path=config_file_path)\n",
    "myScratch.run(\"cv_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "base_config_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batch_64_Epochs_50.yaml',\n",
       " 'Batch_32_Epochs_50.yaml',\n",
       " 'Batch_64_Epochs_20.yaml',\n",
       " 'Batch_16_Epochs_10.yaml',\n",
       " 'Batch_16_Epochs_100.yaml',\n",
       " 'Batch_64_Epochs_100.yaml',\n",
       " 'Batch_32_Epochs_10.yaml',\n",
       " 'Batch_64_Epochs_10.yaml',\n",
       " 'Batch_32_Epochs_20.yaml',\n",
       " 'Batch_128_Epochs_100.yaml',\n",
       " 'Batch_128_Epochs_10.yaml',\n",
       " 'Batch_128_Epochs_20.yaml',\n",
       " 'Batch_16_Epochs_20.yaml',\n",
       " 'Batch_16_Epochs_50.yaml',\n",
       " 'Batch_32_Epochs_100.yaml',\n",
       " 'Batch_128_Epochs_50.yaml']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file_folder = \"/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/Single_1_Anti_2_Dense_1024_Scratch_Ensemble\"\n",
    "os.listdir(config_file_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step0：Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get PARP config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_config(base_config_index,base_yaml_index,base_config_folder):\n",
    "    config_folder_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])\n",
    "    config_subfolder = os.path.join(base_config_folder,config_folder_dic[base_config_index])\n",
    "    yaml_folder = sorted([ele for ele in os.listdir(config_subfolder) if ele.endswith(\".yaml\")])\n",
    "    \n",
    "    base_yaml_path = os.path.join(config_subfolder,yaml_folder[base_yaml_index])\n",
    "    config = get_cfg_defaults()\n",
    "    config.merge_from_file(base_yaml_path)\n",
    "    return config,base_yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "base_config_index = 0 \n",
    "base_yaml_index = 0 \n",
    "base_config,base_yaml_path = get_base_config(base_config_index,base_yaml_index,base_config_folder)\n",
    "del base_config_folder,base_config_index,base_yaml_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epochs_100_Batch_128/Seed_100'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_config.SYSTEM.SAVEFOLDER.split(\"Single_1_Anti_2_Dense_1024/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get Shelburn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shel_config(config_path):\n",
    "    config = get_shelburne_defaults()\n",
    "    config.merge_from_file(config_path)\n",
    "    os.makedirs(config.SYSTEM.SAVEFOLDER,exist_ok=True)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_file_path = \"/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/Scratchtemplate.yaml\"\n",
    "shel_config = get_shel_config(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shel_config.SYSTEM.SAVEFOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shel_config.ScratchTrain.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        config = get_shelburne_defaults()\n",
    "        config.merge_from_file(config_file_path)\n",
    "        batch = config.LRPTTrain.BATCH_SIZE\n",
    "        epochs = config.LRPTTrain.EPOCHS\n",
    "        tempname = \"Epochs_\"+str(epochs)+\"_Batch_\"+str(batch)\n",
    "        ## return\n",
    "        self.shel_config = config\n",
    "        self.save_folder = os.path.join(config.SYSTEM.SAVEFOLDER,self.parp_folder,tempname)\n",
    "        os.makedirs(self.save_folder,exist_ok=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Get shel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_and_data(shel_config):\n",
    "    ######## get data \n",
    "    Variants,Antibiotics,Results = open_pickle(shel_config.SHELDATA.ThreeSets)\n",
    "    DataInfo = pd.read_csv(shel_config.SHELDATA.Info)\n",
    "    Shel_X = pd.concat([Variants,Antibiotics],axis=1)\n",
    "    shel_data = {\"Variants\":Variants,\n",
    "                 \"Antibiotics\":Antibiotics,\n",
    "                 \"Results\":Results,\n",
    "                 \"X\":Shel_X,\n",
    "                 \"Info\":DataInfo}\n",
    "    \n",
    "    ######### get cv \n",
    "    Totaly = Results\n",
    "    cv_split = KFold(n_splits=shel_config.LRPTTrain.CVSPLIT,\n",
    "                     shuffle=shel_config.LRPTTrain.CVSHUFFLE,\n",
    "                     random_state=shel_config.LRPTTrain.CVSEED)\n",
    "    for cv_index,(train_ix,test_ix) in enumerate(cv_split.split(Totaly)):\n",
    "        cv_save_folder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,\"cv_\"+str(cv_index))\n",
    "        os.makedirs(cv_save_folder,exist_ok=True) # make the dircitiony \n",
    "        split_index = {\"train_ix\":train_ix,\"test_ix\":test_ix}\n",
    "        save_as_pickle(split_index,os.path.join(cv_save_folder,\"train_test_ix.pkl\"))\n",
    "    return shel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shel_data = get_cv_and_data(shel_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Get base model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parp_folder =\"/work/PCDC/s202005/Research/PARP/Results/PARPforLRPT/Single_1_Anti_2_Dense_1024\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model_path(parp_folder,base_yaml_path):\n",
    "    epochsname  = os.path.split(os.path.split(base_yaml_path)[0])[1]\n",
    "    seedname = os.path.split(base_yaml_path)[1].replace(\".yaml\",\"\")\n",
    "    trained_model_weights_path = os.path.join(parp_folder,epochsname,seedname,\"modelweights.h5\")\n",
    "    return trained_model_weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model_weights_path = get_base_model_path(parp_folder,base_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build up LRPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Get PARP Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrap_PARP_data(base_config):\n",
    "    # get train/test/or validation for training process\n",
    "    Variants,Antibiotics,Results = open_pickle(base_config.PARPData.TrainPath)\n",
    "    PARP_Train_X = pd.concat([Variants,Antibiotics],1)\n",
    "    PARP_Train_X = PARP_Train_X.iloc[:5,:]  \n",
    "    return PARP_Train_X,Variants.shape[1],Antibiotics.shape[1]\n",
    "PARP_Train_X,VARIANTS_INPUT_DIM, ANTI_INPUT_DIM = wrap_PARP_data(base_config)\n",
    "opts = [\"PARP.VARIANTS_INPUT_DIM\", VARIANTS_INPUT_DIM,\n",
    "        \"PARP.ANTI_INPUT_DIM\", ANTI_INPUT_DIM]\n",
    "base_config.merge_from_list(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Get PARP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model_PARP(base_config):\n",
    "    ## step 3: get model \n",
    "    model = build_parp_model(base_config)\n",
    "    # ## step 4: model compile \n",
    "    loss_key = base_config.PARPTrain.LOSS\n",
    "    loss = [ele for ele in base_config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "    optimizer_key = base_config.PARPTrain.OPTIMIZER\n",
    "    optimizer_rate = base_config.PARPTrain.OPTIMIZERATE\n",
    "    optimizer = [ele for ele in base_config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "    metrics_threshold_list = base_config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    backend.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:05:20.234239: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-12 12:05:20.234482: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-12 12:05:20.240102: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "base_PARP_model = get_base_model_PARP(base_config)\n",
    "base_PARP_model.load_weights(trained_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: build another PARP model with different variants dim-- LRP_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Scratch_base_config = copy.deepcopy(base_config)\n",
    "opts = [\"PARP.VARIANTS_INPUT_DIM\", shel_data[\"Variants\"].shape[1],\n",
    "        \"PARP.ANTI_INPUT_DIM\", shel_data[\"Antibiotics\"].shape[1]]\n",
    "Scratch_base_config.merge_from_list(opts)\n",
    "base_Scratch_model = get_base_model_PARP(Scratch_base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4:Evaluate on Base PARP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_basePARP(base_PARP_model,cv_folder,base_config,shel_config,shel_data,PARP_Train_X):\n",
    "    assert cv_folder in os.listdir(shel_config.SYSTEM.SAVEFOLDER),\"Cv folder doesn't exist\"\n",
    "    cv_ix = open_pickle(os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder,\"train_test_ix.pkl\"))\n",
    "    \n",
    "    ## get save folder\n",
    "    savefolder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder)\n",
    "    train_ix = cv_ix[\"train_ix\"]\n",
    "    test_ix = cv_ix[\"test_ix\"]\n",
    "    \n",
    "    ## get \n",
    "    X = shel_data[\"X\"]\n",
    "    X = Modify_Variants_Mute(X,PARP_Train_X)\n",
    "    \n",
    "    Y = shel_data[\"Results\"]\n",
    "    \n",
    "    ## get train and test \n",
    "    train_x = X.to_numpy()[train_ix]\n",
    "    test_x = X.to_numpy()[test_ix]\n",
    "    \n",
    "    train_y = Y.to_numpy()[train_ix]\n",
    "    test_y = Y.to_numpy()[test_ix]\n",
    "    \n",
    "    ## evaluate \n",
    "    train_score = base_PARP_model.evaluate(train_x,train_y,verbose=0)\n",
    "    test_score = base_PARP_model.evaluate(test_x,test_y,verbose=0)\n",
    "    \n",
    "    train_pred = base_PARP_model.predict(train_x,verbose=0)\n",
    "    test_pred = base_PARP_model.predict(test_x,verbose=0)\n",
    "    \n",
    "    score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "    pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "    \n",
    "    ## save \n",
    "    save_as_pickle(data=score,path = os.path.join(savefolder,\"basePARP_score.pkl\"))\n",
    "    save_as_pickle(data=pred,path = os.path.join(savefolder,\"basePARP_pred.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:05:41.040898: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-12 12:05:41.041393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500015000 Hz\n"
     ]
    }
   ],
   "source": [
    "eval_on_basePARP(base_PARP_model=base_PARP_model,\n",
    "                 cv_folder=\"cv_0\",base_config=base_config,\n",
    "                 shel_config=shel_config,shel_data=shel_data,\n",
    "                 PARP_Train_X = PARP_Train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5:Fit Scratch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval_Scratch(base_Scratch_model,cv_folder,base_config,shel_config,shel_data):\n",
    "    assert cv_folder in os.listdir(shel_config.SYSTEM.SAVEFOLDER),\"Cv folder doesn't exist\"\n",
    "    cv_ix = open_pickle(os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder,\"train_test_ix.pkl\"))\n",
    "    \n",
    "    ## get save folder\n",
    "    savefolder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder)\n",
    "    \n",
    "    train_x = shel_data[\"X\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "    train_y = shel_data[\"Results\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "    \n",
    "    test_x = shel_data[\"X\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "    test_y = shel_data[\"Results\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "    \n",
    "    \n",
    "    log_dir = os.path.join(savefolder,\"tensorboard/logs\")\n",
    "    \n",
    "    if shel_config.ScratchTrain.VALSPLIT is not None:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=shel_config.ScratchTrain.EarlyStoppingpatience, \n",
    "                                         min_delta=shel_config.ScratchTrain.EarlyStoppingdelta),\n",
    "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "        indices = range(len(train_x))\n",
    "        train_x, val_x, train_y, val_y,train_idx,val_idx = train_test_split(train_x,train_y,indices,\n",
    "                                                                            test_size=shel_config.ScratchTrain.VALSIZE,\n",
    "                                                                            shuffle=shel_config.ScratchTrain.VALSPLITSHUFFLE,\n",
    "                                                                            random_state=shel_config.ScratchTrain.VALSEED)\n",
    "        \n",
    "        history = base_Scratch_model.fit(x=train_x,y=train_y,\n",
    "                                         batch_size=shel_config.ScratchTrain.BATCH_SIZE,\n",
    "                                         epochs=shel_config.ScratchTrain.EPOCHS,\n",
    "                                         verbose=shel_config.ScratchTrain.VERBOSE,\n",
    "                                         validation_data=(val_x,val_y),\n",
    "                                         callbacks=callbacks)\n",
    "    else:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                 patience=shel_config.ScratchTrain.EarlyStoppingpatience, \n",
    "                                 min_delta=shel_config.ScratchTrain.EarlyStoppingdelta),\n",
    "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "        history = base_Scratch_model.fit(x=train_x,y=train_y,\n",
    "                                 batch_size=shel_config.ScratchTrain.BATCH_SIZE,\n",
    "                                 epochs=shel_config.ScratchTrain.EPOCHS,\n",
    "                                 verbose=shel_config.ScratchTrain.VERBOSE,\n",
    "                                 callbacks = callbacks)\n",
    "    ## save history \n",
    "    temp_path = os.path.join(savefolder,shel_config.ScratchTrain.MODEL_HISTORY_PATH)\n",
    "    save_as_pickle(data =history.history,path = temp_path)\n",
    "    \n",
    "    ## save model \n",
    "    h5_file = os.path.join(savefolder,shel_config.ScratchTrain.MODEL_WEIGHTS_PATH)\n",
    "    base_Scratch_model.save_weights(h5_file)\n",
    "    \n",
    "    ## save train and val idx \n",
    "    if train_idx:\n",
    "        split_index = {\"train_ix\":train_idx,\"val_ix\":val_idx}\n",
    "        save_as_pickle(split_index,os.path.join(savefolder,\"fit_train_val_ix.pkl\"))\n",
    "    \n",
    "    ## evaluate and predict \n",
    "    train_score = base_Scratch_model.evaluate(train_x,train_y,verbose=0)\n",
    "    train_pred = base_Scratch_model.predict(train_x,verbose=0)\n",
    "    \n",
    "    test_score = base_Scratch_model.evaluate(test_x,test_y,verbose=0)\n",
    "    test_pred = base_Scratch_model.predict(test_x,verbose=0)\n",
    "    \n",
    "    score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "    pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "    \n",
    "    if train_idx:\n",
    "        val_score = base_Scratch_model.evaluate(val_x,val_y,verbose=0)\n",
    "        val_pred = base_Scratch_model.predict(val_x,verbose=0)\n",
    "        score[\"val_score\"] = val_score\n",
    "        pred[\"val_pred\"] = val_pred\n",
    "    \n",
    "    save_as_pickle(data=score,path = os.path.join(savefolder,\"fit_score.pkl\"))\n",
    "    save_as_pickle(data=pred,path = os.path.join(savefolder,\"fit_pred.pkl\"))\n",
    "    return base_Scratch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:07:17.333642: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-04-12 12:07:17.333673: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-04-12 12:07:17.333717: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-04-12 12:07:21.062925: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-04-12 12:07:21.063167: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-04-12 12:07:21.151073: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-04-12 12:07:21.170585: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-04-12 12:07:21.198278: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21\n",
      "2022-04-12 12:07:21.209978: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.trace.json.gz\n",
      "2022-04-12 12:07:21.288144: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21\n",
      "2022-04-12 12:07:21.298719: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.memory_profile.json.gz\n",
      "2022-04-12 12:07:21.303442: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21Dumped tool data for xplane.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/Scratch/Single_1_Anti_2_Dense_1024_Ensemble/cv_0/tensorboard/logs/train/plugins/profile/2022_04_12_12_07_21/Nucleus121.kernel_stats.pb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_Scratch_model = train_and_eval_Scratch(base_Scratch_model=base_Scratch_model,\n",
    "                        cv_folder=\"cv_0\",\n",
    "                        base_config=base_config,\n",
    "                        shel_config=shel_config,\n",
    "                        shel_data=shel_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
