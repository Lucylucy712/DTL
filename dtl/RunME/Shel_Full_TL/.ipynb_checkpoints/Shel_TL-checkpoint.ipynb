{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes\")\n",
    "from config.sheldefaults import *\n",
    "# config.basedeafults is copied from \"/work/PCDC/s202005/Research/PARP/Codes/PARpforLRPT/config/defaults\"\n",
    "from config.basedefaults import *\n",
    "from general.commonfuns import * \n",
    "from general.imports import *\n",
    "from general.logs import * \n",
    "from data.data_legos import * \n",
    "from models.TL.TL import *\n",
    "os.chdir(\"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT\")\n",
    "from model.PARPModel import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShelTL(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _get_base_config(self,base_config_index,base_yaml_index,base_config_folder):\n",
    "        config_folder_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])\n",
    "        config_subfolder = os.path.join(base_config_folder,config_folder_dic[base_config_index])\n",
    "        yaml_folder = sorted([ele for ele in os.listdir(config_subfolder) if ele.endswith(\".yaml\")])\n",
    "        base_yaml_path = os.path.join(config_subfolder,yaml_folder[base_yaml_index])\n",
    "        config = get_cfg_defaults()\n",
    "        config.merge_from_file(base_yaml_path)\n",
    "        \n",
    "        ## return \n",
    "        self.base_config = config\n",
    "        self.base_yaml_path = base_yaml_path\n",
    "        self.parp_folder = config.SYSTEM.SAVEFOLDER.split(\"Single_1_Anti_2_Dense_1024/\")[-1]\n",
    "        \n",
    "        \n",
    "    def _get_shel_config(self,config_file_path):\n",
    "        config = get_shelburne_defaults()\n",
    "        config.merge_from_file(config_file_path)\n",
    "        fin_ver = config.TL.FINAL_DENSE_VERSION\n",
    "        batch = config.TLTrain.BATCH_SIZE\n",
    "        epochs = config.TLTrain.EPOCHS\n",
    "        tempname = fin_ver+\"_Epochs_\"+str(epochs)+\"_Batch_\"+str(batch)\n",
    "        ## return\n",
    "        self.shel_config = config\n",
    "        self.save_folder = os.path.join(config.SYSTEM.SAVEFOLDER,self.parp_folder,tempname)\n",
    "        os.makedirs(self.save_folder,exist_ok=True)\n",
    "        \n",
    "    def _get_cv_and_data(self):\n",
    "        shel_config = self.shel_config\n",
    "        ######## get data \n",
    "        Variants,Antibiotics,Results = open_pickle(shel_config.SHELDATA.ThreeSets)\n",
    "        DataInfo = pd.read_csv(shel_config.SHELDATA.Info)\n",
    "        Shel_X = pd.concat([Variants,Antibiotics],axis=1)\n",
    "        shel_data = {\"Variants\":Variants,\n",
    "                     \"Antibiotics\":Antibiotics,\n",
    "                     \"Results\":Results,\n",
    "                     \"X\":Shel_X,\n",
    "                     \"Info\":DataInfo}\n",
    "\n",
    "        ######### get cv \n",
    "        Totaly = Results\n",
    "        cv_split = KFold(n_splits=shel_config.LRPTTrain.CVSPLIT,\n",
    "                         shuffle=shel_config.LRPTTrain.CVSHUFFLE,\n",
    "                         random_state=shel_config.LRPTTrain.CVSEED)\n",
    "        for cv_index,(train_ix,test_ix) in enumerate(cv_split.split(Totaly)):\n",
    "            cv_save_folder = os.path.join(self.save_folder,\"cv_\"+str(cv_index))\n",
    "            os.makedirs(cv_save_folder,exist_ok=True) # make the dircitiony \n",
    "            split_index = {\"train_ix\":train_ix,\"test_ix\":test_ix}\n",
    "            save_as_pickle(split_index,os.path.join(cv_save_folder,\"train_test_ix.pkl\"))\n",
    "            \n",
    "        ## return \n",
    "        self.shel_data = shel_data\n",
    "    \n",
    "    def _get_base_model_path(self,parp_folder):\n",
    "        \n",
    "        epochsname  = os.path.split(os.path.split(self.base_yaml_path)[0])[1]\n",
    "        seedname = os.path.split(self.base_yaml_path)[1].replace(\".yaml\",\"\")\n",
    "        trained_model_weights_path = os.path.join(parp_folder,epochsname,seedname,\"modelweights.h5\")\n",
    "        ## return \n",
    "        self.trained_model_weights_path = trained_model_weights_path\n",
    "        \n",
    "    def _wrap_PARP_data(self):\n",
    "        # get train/test/or validation for training process\n",
    "        Variants,Antibiotics,Results = open_pickle(self.base_config.PARPData.TrainPath)\n",
    "        PARP_Train_X = pd.concat([Variants,Antibiotics],1)\n",
    "        \n",
    "        # return \n",
    "        self.PARP_Train_X = PARP_Train_X.iloc[:5,:]  \n",
    "        \n",
    "        opts = [\"PARP.VARIANTS_INPUT_DIM\", Variants.shape[1],\n",
    "                \"PARP.ANTI_INPUT_DIM\", Antibiotics.shape[1]]\n",
    "        self.base_config.merge_from_list(opts)\n",
    "        \n",
    "    def _get_base_model_PARP(self):\n",
    "\n",
    "        model = build_parp_model(self.base_config)\n",
    "        # ## step 4: model compile \n",
    "        loss_key = self.base_config.PARPTrain.LOSS\n",
    "        loss = [ele for ele in self.base_config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "        optimizer_key = self.base_config.PARPTrain.OPTIMIZER\n",
    "        optimizer_rate = self.base_config.PARPTrain.OPTIMIZERATE\n",
    "        optimizer = [ele for ele in self.base_config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "        metrics_threshold_list = self.base_config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "        metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,\n",
    "                                                   name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        backend.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "        \n",
    "\n",
    "        model.load_weights(self.trained_model_weights_path)\n",
    "        self.PARP_base_model = model \n",
    "        \n",
    "    def _get_TL_model(self):\n",
    "        TL_base_config = copy.deepcopy(self.base_config)\n",
    "        TL_base_model = build_parp_model(TL_base_config)\n",
    "        TL_base_model.load_weights(self.trained_model_weights_path)\n",
    "        \n",
    "        TL_model = build_TL_model(self.shel_config,TL_base_model)\n",
    "        TL_model = compile_TL_model(TL_model,TL_base_config)\n",
    "        self.TL_model = TL_model\n",
    "    \n",
    "    def eval_on_basePARP(self,cv_folder):\n",
    "        \n",
    "        assert cv_folder in os.listdir(self.save_folder),\"Cv folder doesn't exist\"\n",
    "        cv_ix = open_pickle(os.path.join(self.save_folder,cv_folder,\"train_test_ix.pkl\"))\n",
    "\n",
    "        ## get save folder\n",
    "        savefolder = os.path.join(self.save_folder,cv_folder)\n",
    "        train_ix = cv_ix[\"train_ix\"]\n",
    "        test_ix = cv_ix[\"test_ix\"]\n",
    "\n",
    "        ## get \n",
    "        X = self.shel_data[\"X\"]\n",
    "        X = Modify_Variants_Mute(X,self.PARP_Train_X)\n",
    "\n",
    "        Y = self.shel_data[\"Results\"]\n",
    "\n",
    "        ## get train and test \n",
    "        train_x = X.to_numpy()[train_ix]\n",
    "        test_x = X.to_numpy()[test_ix]\n",
    "\n",
    "        train_y = Y.to_numpy()[train_ix]\n",
    "        test_y = Y.to_numpy()[test_ix]\n",
    "\n",
    "        ## evaluate \n",
    "        train_score = self.PARP_base_model.evaluate(train_x,train_y,verbose=0)\n",
    "        test_score = self.PARP_base_model.evaluate(test_x,test_y,verbose=0)\n",
    "\n",
    "        train_pred = self.PARP_base_model.predict(train_x,verbose=0)\n",
    "        test_pred =self.PARP_base_model.predict(test_x,verbose=0)\n",
    "\n",
    "        score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "        pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "\n",
    "        ## save \n",
    "        save_as_pickle(data=score,path = os.path.join(savefolder,\"basePARP_score.pkl\"))\n",
    "        save_as_pickle(data=pred,path = os.path.join(savefolder,\"basePARP_pred.pkl\"))\n",
    "    \n",
    "        \n",
    "    def train_and_eval_TL(self,cv_folder):\n",
    "        assert cv_folder in os.listdir(self.save_folder),\"Cv folder doesn't exist\"\n",
    "        cv_ix = open_pickle(os.path.join(self.save_folder,cv_folder,\"train_test_ix.pkl\"))\n",
    "\n",
    "        ## get save folder\n",
    "        savefolder = os.path.join(self.save_folder,cv_folder)\n",
    "        X = Modify_Variants_Mute(self.shel_data[\"X\"],self.PARP_Train_X)\n",
    "\n",
    "        train_x = X.to_numpy()[cv_ix[\"train_ix\"]]\n",
    "        train_y = self.shel_data[\"Results\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "\n",
    "        test_x = X.to_numpy()[cv_ix[\"test_ix\"]]\n",
    "        test_y = self.shel_data[\"Results\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "\n",
    "\n",
    "        log_dir = os.path.join(savefolder,\"tensorboard/logs\")\n",
    "\n",
    "        if self.shel_config.TLTrain.VALSPLIT is not None:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             patience=self.shel_config.TLTrain.EarlyStoppingpatience, \n",
    "                                             min_delta=self.shel_config.TLTrain.EarlyStoppingdelta),\n",
    "                         tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "            indices = range(len(train_x))\n",
    "            train_x, val_x, train_y, val_y,train_idx,val_idx = train_test_split(train_x,train_y,indices,\n",
    "                                                                                test_size=self.shel_config.TLTrain.VALSIZE,\n",
    "                                                                                shuffle=self.shel_config.TLTrain.VALSPLITSHUFFLE,\n",
    "                                                                                random_state=self.shel_config.TLTrain.VALSEED)\n",
    "\n",
    "            history = self.TL_model.fit(x=train_x,y=train_y,\n",
    "                                          batch_size=self.shel_config.TLTrain.BATCH_SIZE,\n",
    "                                          epochs=self.shel_config.TLTrain.EPOCHS,\n",
    "                                          verbose=self.shel_config.TLTrain.VERBOSE,\n",
    "                                          validation_data=(val_x,val_y),\n",
    "                                          callbacks=callbacks)\n",
    "        else:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                     patience=self.shel_config.TLTrain.EarlyStoppingpatience, \n",
    "                                     min_delta=self.shel_config.TLTrain.EarlyStoppingdelta),\n",
    "                         tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "            history = self.TL_model.fit(x=train_x,y=train_y,\n",
    "                                          batch_size=self.shel_config.TLTrain.BATCH_SIZE,\n",
    "                                          epochs=self.shel_config.TLTrain.EPOCHS,\n",
    "                                          verbose=self.shel_config.TLTrain.VERBOSE,\n",
    "                                          callbacks = callbacks)\n",
    "        ## save history \n",
    "        temp_path = os.path.join(savefolder,self.shel_config.TLTrain.MODEL_HISTORY_PATH)\n",
    "        save_as_pickle(data =history.history,path = temp_path)\n",
    "\n",
    "        ## save model \n",
    "        h5_file = os.path.join(savefolder,self.shel_config.TLTrain.MODEL_WEIGHTS_PATH)\n",
    "        self.TL_model.save_weights(h5_file)\n",
    "\n",
    "        ## save train and val idx \n",
    "        if train_idx:\n",
    "            split_index = {\"train_ix\":train_idx,\"val_ix\":val_idx}\n",
    "            save_as_pickle(split_index,os.path.join(savefolder,\"fit_train_val_ix.pkl\"))\n",
    "\n",
    "        ## evaluate and predict \n",
    "        train_score = self.TL_model.evaluate(train_x,train_y,verbose=0)\n",
    "        train_pred = self.TL_model.predict(train_x,verbose=0)\n",
    "\n",
    "        test_score = self.TL_model.evaluate(test_x,test_y,verbose=0)\n",
    "        test_pred = self.TL_model.predict(test_x,verbose=0)\n",
    "\n",
    "        score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "        pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "\n",
    "        if train_idx:\n",
    "            val_score = self.TL_model.evaluate(val_x,val_y,verbose=0)\n",
    "            val_pred = self.TL_model.predict(val_x,verbose=0)\n",
    "            score[\"val_score\"] = val_score\n",
    "            pred[\"val_pred\"] = val_pred\n",
    "\n",
    "        save_as_pickle(data=score,path = os.path.join(savefolder,\"fit_score.pkl\"))\n",
    "        save_as_pickle(data=pred,path = os.path.join(savefolder,\"fit_pred.pkl\"))\n",
    "\n",
    "        \n",
    "        \n",
    "    def build(self,base_config_index,base_yaml_index,config_file_path):\n",
    "        base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "        parp_folder =\"/work/PCDC/s202005/Research/PARP/Results/PARPforLRPT/Single_1_Anti_2_Dense_1024\" \n",
    "        self._get_base_config(base_config_index=base_config_index,\n",
    "                              base_config_folder=base_config_folder,\n",
    "                              base_yaml_index=base_yaml_index)\n",
    "        self._get_shel_config(config_file_path=config_file_path)\n",
    "        self._get_cv_and_data()\n",
    "        self._get_base_model_path(parp_folder=parp_folder)\n",
    "        self._wrap_PARP_data()\n",
    "        self._get_base_model_PARP()\n",
    "        self._get_TL_model()\n",
    "\n",
    "    def run(self,cv_folder):\n",
    "        self.eval_on_basePARP(cv_folder=cv_folder)\n",
    "        self.train_and_eval_TL(cv_folder=cv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_index = 0 \n",
    "base_yaml_index = 0 \n",
    "config_file_path = \"/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/TLtemplate.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTL = ShelTL()\n",
    "myTL.build(base_config_index=base_config_index,base_yaml_index=base_yaml_index,config_file_path=config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 15:48:34.492405: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-03-28 15:48:34.492441: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-03-28 15:48:34.492489: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-03-28 15:48:36.210188: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-03-28 15:48:36.210224: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-03-28 15:48:36.247061: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-03-28 15:48:36.253928: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-03-28 15:48:36.268221: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36\n",
      "2022-03-28 15:48:36.272649: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.trace.json.gz\n",
      "2022-03-28 15:48:36.299335: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36\n",
      "2022-03-28 15:48:36.319238: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.memory_profile.json.gz\n",
      "2022-03-28 15:48:36.347940: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36Dumped tool data for xplane.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Results/TL/Single_1_Anti_2_Dense_1024_Ensemble/Epochs_100_Batch_128/Seed_100/DefaultWeights_Epochs_100_Batch_32/cv_0/tensorboard/logs/train/plugins/profile/2022_03_28_15_48_36/NucleusA189.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ShelTL' object has no attribute 'LRPT_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_127949/1453844920.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyTL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_127949/4261556878.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, cv_folder)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_basePARP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_eval_TL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_127949/4261556878.py\u001b[0m in \u001b[0;36mtrain_and_eval_TL\u001b[0;34m(self, cv_folder)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m## save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mh5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavefolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTLTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_WEIGHTS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLRPT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m## save train and val idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ShelTL' object has no attribute 'LRPT_model'"
     ]
    }
   ],
   "source": [
    "myTL.run(\"cv_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "base_config_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config_file_folder = \"/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/Single_1_Anti_2_Dense_1024_Ensemble\"\n",
    "# os.listdir(config_file_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step0：Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get PARP config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_config(base_config_index,base_yaml_index,base_config_folder):\n",
    "    config_folder_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])\n",
    "    config_subfolder = os.path.join(base_config_folder,config_folder_dic[base_config_index])\n",
    "    yaml_folder = sorted([ele for ele in os.listdir(config_subfolder) if ele.endswith(\".yaml\")])\n",
    "    base_yaml_path = os.path.join(config_subfolder,yaml_folder[base_yaml_index])\n",
    "    config = get_cfg_defaults()\n",
    "    config.merge_from_file(base_yaml_path)\n",
    "    return config,base_yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "base_config_index = 0 \n",
    "base_yaml_index = 0 \n",
    "base_config,base_yaml_path = get_base_config(base_config_index,base_yaml_index,base_config_folder)\n",
    "del base_config_folder,base_config_index,base_yaml_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get Shelburn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shel_config(config_path):\n",
    "    config = get_shelburne_defaults()\n",
    "    config.merge_from_file(config_path)\n",
    "    os.makedirs(config.SYSTEM.SAVEFOLDER,exist_ok=True)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_file_path = \"/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/TLtemplate.yaml\"\n",
    "shel_config = get_shel_config(config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Get shel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_and_data(shel_config):\n",
    "    ######## get data \n",
    "    Variants,Antibiotics,Results = open_pickle(shel_config.SHELDATA.ThreeSets)\n",
    "    DataInfo = pd.read_csv(shel_config.SHELDATA.Info)\n",
    "    Shel_X = pd.concat([Variants,Antibiotics],axis=1)\n",
    "    shel_data = {\"Variants\":Variants,\n",
    "                 \"Antibiotics\":Antibiotics,\n",
    "                 \"Results\":Results,\n",
    "                 \"X\":Shel_X,\n",
    "                 \"Info\":DataInfo}\n",
    "    \n",
    "    ######### get cv \n",
    "    Totaly = Results\n",
    "    cv_split = KFold(n_splits=shel_config.LRPTTrain.CVSPLIT,\n",
    "                     shuffle=shel_config.LRPTTrain.CVSHUFFLE,\n",
    "                     random_state=shel_config.LRPTTrain.CVSEED)\n",
    "    for cv_index,(train_ix,test_ix) in enumerate(cv_split.split(Totaly)):\n",
    "        cv_save_folder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,\"cv_\"+str(cv_index))\n",
    "        os.makedirs(cv_save_folder,exist_ok=True) # make the dircitiony \n",
    "        split_index = {\"train_ix\":train_ix,\"test_ix\":test_ix}\n",
    "        save_as_pickle(split_index,os.path.join(cv_save_folder,\"train_test_ix.pkl\"))\n",
    "    return shel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shel_data = get_cv_and_data(shel_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Get base model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parp_folder =\"/work/PCDC/s202005/Research/PARP/Results/PARPforLRPT/Single_1_Anti_2_Dense_1024\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model_path(parp_folder,base_yaml_path):\n",
    "    epochsname  = os.path.split(os.path.split(base_yaml_path)[0])[1]\n",
    "    seedname = os.path.split(base_yaml_path)[1].replace(\".yaml\",\"\")\n",
    "    trained_model_weights_path = os.path.join(parp_folder,epochsname,seedname,\"modelweights.h5\")\n",
    "    return trained_model_weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model_weights_path = get_base_model_path(parp_folder,base_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build up LRPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Get PARP Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrap_PARP_data(base_config):\n",
    "    # get train/test/or validation for training process\n",
    "    Variants,Antibiotics,Results = open_pickle(base_config.PARPData.TrainPath)\n",
    "    PARP_Train_X = pd.concat([Variants,Antibiotics],1)\n",
    "    PARP_Train_X = PARP_Train_X.iloc[:5,:]  \n",
    "    return PARP_Train_X,Variants.shape[1],Antibiotics.shape[1]\n",
    "PARP_Train_X,VARIANTS_INPUT_DIM, ANTI_INPUT_DIM = wrap_PARP_data(base_config)\n",
    "opts = [\"PARP.VARIANTS_INPUT_DIM\", VARIANTS_INPUT_DIM,\n",
    "        \"PARP.ANTI_INPUT_DIM\", ANTI_INPUT_DIM]\n",
    "base_config.merge_from_list(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Get PARP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model_PARP(base_config):\n",
    "    ## step 3: get model \n",
    "    model = build_parp_model(base_config)\n",
    "    # ## step 4: model compile \n",
    "    loss_key = base_config.PARPTrain.LOSS\n",
    "    loss = [ele for ele in base_config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "    optimizer_key = base_config.PARPTrain.OPTIMIZER\n",
    "    optimizer_rate = base_config.PARPTrain.OPTIMIZERATE\n",
    "    optimizer = [ele for ele in base_config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "    metrics_threshold_list = base_config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    backend.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 14:14:54.772989: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-28 14:14:54.773391: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-28 14:14:54.780143: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "base_PARP_model = get_base_model_PARP(base_config)\n",
    "base_PARP_model.load_weights(trained_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: build TL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TL_base_config = copy.deepcopy(base_config)\n",
    "TL_base_model = build_parp_model(TL_base_config)\n",
    "TL_base_model.load_weights(trained_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4: change TL_base_model based on LRPT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TL_model = build_TL_model(shel_config,TL_base_model)\n",
    "TL_model = compile_TL_model(TL_model,TL_base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4:Evaluate on Base PARP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_basePARP(base_PARP_model,cv_folder,base_config,shel_config,shel_data,PARP_Train_X):\n",
    "    assert cv_folder in os.listdir(shel_config.SYSTEM.SAVEFOLDER),\"Cv folder doesn't exist\"\n",
    "    cv_ix = open_pickle(os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder,\"train_test_ix.pkl\"))\n",
    "    \n",
    "    ## get save folder\n",
    "    savefolder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder)\n",
    "    train_ix = cv_ix[\"train_ix\"]\n",
    "    test_ix = cv_ix[\"test_ix\"]\n",
    "    \n",
    "    ## get \n",
    "    X = shel_data[\"X\"]\n",
    "    X = Modify_Variants_Mute(X,PARP_Train_X)\n",
    "    \n",
    "    Y = shel_data[\"Results\"]\n",
    "    \n",
    "    ## get train and test \n",
    "    train_x = X.to_numpy()[train_ix]\n",
    "    test_x = X.to_numpy()[test_ix]\n",
    "    \n",
    "    train_y = Y.to_numpy()[train_ix]\n",
    "    test_y = Y.to_numpy()[test_ix]\n",
    "    \n",
    "    ## evaluate \n",
    "    train_score = base_PARP_model.evaluate(train_x,train_y,verbose=0)\n",
    "    test_score = base_PARP_model.evaluate(test_x,test_y,verbose=0)\n",
    "    \n",
    "    train_pred = base_PARP_model.predict(train_x,verbose=0)\n",
    "    test_pred = base_PARP_model.predict(test_x,verbose=0)\n",
    "    \n",
    "    score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "    pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "    \n",
    "    ## save \n",
    "    save_as_pickle(data=score,path = os.path.join(savefolder,\"basePARP_score.pkl\"))\n",
    "    save_as_pickle(data=pred,path = os.path.join(savefolder,\"basePARP_pred.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 14:15:54.879882: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-03-28 14:15:54.880452: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2700115000 Hz\n"
     ]
    }
   ],
   "source": [
    "eval_on_basePARP(base_PARP_model=base_PARP_model,\n",
    "                 cv_folder=\"cv_0\",base_config=base_config,\n",
    "                 shel_config=shel_config,shel_data=shel_data,\n",
    "                 PARP_Train_X = PARP_Train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5:Fit LRPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval_TL(TL_model,cv_folder,base_config,shel_config,shel_data,PARP_Train_X):\n",
    "    assert cv_folder in os.listdir(shel_config.SYSTEM.SAVEFOLDER),\"Cv folder doesn't exist\"\n",
    "    cv_ix = open_pickle(os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder,\"train_test_ix.pkl\"))\n",
    "    \n",
    "    ## get save folder\n",
    "    savefolder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder)\n",
    "    \n",
    "    X = Modify_Variants_Mute(shel_data[\"X\"],PARP_Train_X)\n",
    "    \n",
    "    train_x = X.to_numpy()[cv_ix[\"train_ix\"]]\n",
    "    train_y = shel_data[\"Results\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "    \n",
    "    test_x = X.to_numpy()[cv_ix[\"test_ix\"]]\n",
    "    test_y = shel_data[\"Results\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "    \n",
    "    \n",
    "    log_dir = os.path.join(savefolder,\"tensorboard/logs\")\n",
    "    \n",
    "    if shel_config.LRPTTrain.VALSPLIT is not None:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=shel_config.LRPTTrain.EarlyStoppingpatience, \n",
    "                                         min_delta=shel_config.LRPTTrain.EarlyStoppingdelta),\n",
    "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "        indices = range(len(train_x))\n",
    "        train_x, val_x, train_y, val_y,train_idx,val_idx = train_test_split(train_x,train_y,indices,\n",
    "                                                                            test_size=shel_config.LRPTTrain.VALSIZE,\n",
    "                                                                            shuffle=shel_config.LRPTTrain.VALSPLITSHUFFLE,\n",
    "                                                                            random_state=shel_config.LRPTTrain.VALSEED)\n",
    "        \n",
    "        history = TL_model.fit(x=train_x,y=train_y,\n",
    "                                 batch_size=shel_config.LRPTTrain.BATCH_SIZE,\n",
    "                                 epochs=shel_config.LRPTTrain.EPOCHS,\n",
    "                                 verbose=shel_config.LRPTTrain.VERBOSE,\n",
    "                                 validation_data=(val_x,val_y),\n",
    "                                 callbacks=callbacks)\n",
    "    else:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                 patience=shel_config.LRPTTrain.EarlyStoppingpatience, \n",
    "                                 min_delta=shel_config.LRPTTrain.EarlyStoppingdelta),\n",
    "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "        history = TL_model.fit(x=train_x,y=train_y,\n",
    "                                 batch_size=shel_config.LRPTTrain.BATCH_SIZE,\n",
    "                                 epochs=shel_config.LRPTTrain.EPOCHS,\n",
    "                                 verbose=shel_config.LRPTTrain.VERBOSE,\n",
    "                                 callbacks = callbacks)\n",
    "    ## save history \n",
    "    temp_path = os.path.join(savefolder,shel_config.LRPTTrain.MODEL_HISTORY_PATH)\n",
    "    save_as_pickle(data =history.history,path = temp_path)\n",
    "    \n",
    "    ## save model \n",
    "    h5_file = os.path.join(savefolder,shel_config.LRPTTrain.MODEL_WEIGHTS_PATH)\n",
    "    TL_model.save_weights(h5_file)\n",
    "    \n",
    "    ## save train and val idx \n",
    "    if train_idx:\n",
    "        split_index = {\"train_ix\":train_idx,\"val_ix\":val_idx}\n",
    "        save_as_pickle(split_index,os.path.join(savefolder,\"fit_train_val_ix.pkl\"))\n",
    "    \n",
    "    ## evaluate and predict \n",
    "    train_score = TL_model.evaluate(train_x,train_y,verbose=0)\n",
    "    train_pred = TL_model.predict(train_x,verbose=0)\n",
    "    \n",
    "    test_score = TL_model.evaluate(test_x,test_y,verbose=0)\n",
    "    test_pred = TL_model.predict(test_x,verbose=0)\n",
    "    \n",
    "    score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "    pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "    \n",
    "    if train_idx:\n",
    "        val_score = TL_model.evaluate(val_x,val_y,verbose=0)\n",
    "        val_pred = TL_model.predict(val_x,verbose=0)\n",
    "        score[\"val_score\"] = val_score\n",
    "        pred[\"val_pred\"] = val_pred\n",
    "    \n",
    "    save_as_pickle(data=score,path = os.path.join(savefolder,\"fit_score.pkl\"))\n",
    "    save_as_pickle(data=pred,path = os.path.join(savefolder,\"fit_pred.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 14:23:21.733779: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-03-28 14:23:21.733821: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-03-28 14:23:21.733908: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115906/4259417345.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m TL_model = train_and_eval_TL(TL_model=TL_model,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mcv_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cv_0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mbase_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mshel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         shel_data=shel_data,PARP_Train_X = PARP_Train_X)\n",
      "\u001b[0;32m/tmp/ipykernel_115906/3096760412.py\u001b[0m in \u001b[0;36mtrain_and_eval_TL\u001b[0;34m(TL_model, cv_folder, base_config, shel_config, shel_data, PARP_Train_X)\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                             random_state=shel_config.LRPTTrain.VALSEED)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         history = TL_model.fit(x=train_x,y=train_y,\n\u001b[0m\u001b[1;32m     31\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLRPTTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLRPTTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "TL_model = train_and_eval_TL(TL_model=TL_model,\n",
    "                        cv_folder=\"cv_0\",\n",
    "                        base_config=base_config,\n",
    "                        shel_config=shel_config,\n",
    "                        shel_data=shel_data,PARP_Train_X = PARP_Train_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
