{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 23:08:01.542342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/16.05.8/lib64/slurm:/cm/shared/apps/slurm/16.05.8/lib64\n",
      "2022-03-26 23:08:01.542383: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.chdir(\"/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes\")\n",
    "from config.sheldefaults import *\n",
    "# config.basedeafults is copied from \"/work/PCDC/s202005/Research/PARP/Codes/PARpforLRPT/config/defaults\"\n",
    "from config.basedefaults import *\n",
    "from general.commonfuns import * \n",
    "from general.imports import *\n",
    "from general.logs import * \n",
    "from data.data_legos import * \n",
    "from models.LRPT.LRPT import *\n",
    "os.chdir(\"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT\")\n",
    "from model.PARPModel import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShelLRPT(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _get_base_config(self,base_config_index,base_yaml_index,base_config_folder):\n",
    "        config_folder_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])\n",
    "        config_subfolder = os.path.join(base_config_folder,config_folder_dic[base_config_index])\n",
    "        yaml_folder = sorted([ele for ele in os.listdir(config_subfolder) if ele.endswith(\".yaml\")])\n",
    "\n",
    "        base_yaml_path = os.path.join(config_subfolder,yaml_folder[base_yaml_index])\n",
    "        config = get_cfg_defaults()\n",
    "        config.merge_from_file(base_yaml_path)\n",
    "        \n",
    "        ## return \n",
    "        self.base_config = config\n",
    "        self.base_yaml_path = base_yaml_path\n",
    "        self.parp_folder = config.SYSTEM.SAVEFOLDER.split(\"Single_1_Anti_2_Dense_1024/\")[-1]\n",
    "\n",
    "    def _get_shel_config(self,config_file_path):\n",
    "        config = get_shelburne_defaults()\n",
    "        config.merge_from_file(config_file_path)\n",
    "        var_ver = config.LRPT.FINAL_DENSE_VERSION\n",
    "        fin_ver = config.LRPT.FINAL_DENSE_VERSION\n",
    "        batch = config.LRPTTrain.BATCH_SIZE\n",
    "        epochs = config.LRPTTrain.EPOCHS\n",
    "        tempname = var_ver+\"_\"+fin_ver+\"_Epochs_\"+str(epochs)+\"_Batch_\"+str(batch)\n",
    "        ## return\n",
    "        self.shel_config = config\n",
    "        self.save_folder = os.path.join(config.SYSTEM.SAVEFOLDER,self.parp_folder,tempname)\n",
    "        os.makedirs(self.save_folder,exist_ok=True)\n",
    "        \n",
    "    def _get_cv_and_data(self):\n",
    "        shel_config = self.shel_config\n",
    "        ######## get data \n",
    "        Variants,Antibiotics,Results = open_pickle(shel_config.SHELDATA.ThreeSets)\n",
    "        DataInfo = pd.read_csv(shel_config.SHELDATA.Info)\n",
    "        Shel_X = pd.concat([Variants,Antibiotics],axis=1)\n",
    "        shel_data = {\"Variants\":Variants,\n",
    "                     \"Antibiotics\":Antibiotics,\n",
    "                     \"Results\":Results,\n",
    "                     \"X\":Shel_X,\n",
    "                     \"Info\":DataInfo}\n",
    "\n",
    "        ######### get cv \n",
    "        Totaly = Results\n",
    "        cv_split = KFold(n_splits=shel_config.LRPTTrain.CVSPLIT,\n",
    "                         shuffle=shel_config.LRPTTrain.CVSHUFFLE,\n",
    "                         random_state=shel_config.LRPTTrain.CVSEED)\n",
    "        for cv_index,(train_ix,test_ix) in enumerate(cv_split.split(Totaly)):\n",
    "            cv_save_folder = os.path.join(self.save_folder,\"cv_\"+str(cv_index))\n",
    "            os.makedirs(cv_save_folder,exist_ok=True) # make the dircitiony \n",
    "            split_index = {\"train_ix\":train_ix,\"test_ix\":test_ix}\n",
    "            save_as_pickle(split_index,os.path.join(cv_save_folder,\"train_test_ix.pkl\"))\n",
    "            \n",
    "        ## return \n",
    "        self.shel_data = shel_data\n",
    "    \n",
    "    def _get_base_model_path(self,parp_folder):\n",
    "        \n",
    "        epochsname  = os.path.split(os.path.split(self.base_yaml_path)[0])[1]\n",
    "        seedname = os.path.split(self.base_yaml_path)[1].replace(\".yaml\",\"\")\n",
    "        trained_model_weights_path = os.path.join(parp_folder,epochsname,seedname,\"modelweights.h5\")\n",
    "        ## return \n",
    "        self.trained_model_weights_path = trained_model_weights_path\n",
    "        \n",
    "    def _wrap_PARP_data(self):\n",
    "        # get train/test/or validation for training process\n",
    "        Variants,Antibiotics,Results = open_pickle(self.base_config.PARPData.TrainPath)\n",
    "        PARP_Train_X = pd.concat([Variants,Antibiotics],1)\n",
    "        \n",
    "        # return \n",
    "        self.PARP_Train_X = PARP_Train_X.iloc[:5,:]  \n",
    "        \n",
    "        opts = [\"PARP.VARIANTS_INPUT_DIM\", Variants.shape[1],\n",
    "                \"PARP.ANTI_INPUT_DIM\", Antibiotics.shape[1]]\n",
    "        self.base_config.merge_from_list(opts)\n",
    "        \n",
    "    def _get_base_model_PARP(self):\n",
    "\n",
    "        model = build_parp_model(self.base_config)\n",
    "        # ## step 4: model compile \n",
    "        loss_key = self.base_config.PARPTrain.LOSS\n",
    "        loss = [ele for ele in self.base_config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "        optimizer_key = self.base_config.PARPTrain.OPTIMIZER\n",
    "        optimizer_rate = self.base_config.PARPTrain.OPTIMIZERATE\n",
    "        optimizer = [ele for ele in self.base_config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "        metrics_threshold_list = self.base_config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "        metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,\n",
    "                                                   name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        backend.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "        \n",
    "\n",
    "        model.load_weights(self.trained_model_weights_path)\n",
    "        self.PARP_base_model = model \n",
    "        \n",
    "    def _get_base_model_LRPT(self):\n",
    "        LRPT_base_config = copy.deepcopy(self.base_config)\n",
    "        opts = [\"PARP.VARIANTS_INPUT_DIM\", self.shel_data[\"Variants\"].shape[1],\n",
    "                \"PARP.ANTI_INPUT_DIM\", self.shel_data[\"Antibiotics\"].shape[1]]\n",
    "        LRPT_base_config.merge_from_list(opts)\n",
    "        LRPT_base_model = build_parp_model(LRPT_base_config)\n",
    "        self.LRPT_base_model = LRPT_base_model\n",
    "        self.LRPT_base_config = LRPT_base_config\n",
    "    \n",
    "    def _build_LRPT_model(self):\n",
    "        LRPT_model = build_LRPT_model(self.shel_config,self.LRPT_base_model,\n",
    "                                      self.shel_data[\"X\"],\n",
    "                                      self.PARP_base_model,self.PARP_Train_X)\n",
    "        LRPT_model = compile_LRPT_model(LRPT_model,self.LRPT_base_config)\n",
    "        self.LRPT_model = LRPT_model\n",
    "    \n",
    "    def eval_on_basePARP(self,cv_folder):\n",
    "        \n",
    "        assert cv_folder in os.listdir(self.save_folder),\"Cv folder doesn't exist\"\n",
    "        cv_ix = open_pickle(os.path.join(self.save_folder,cv_folder,\"train_test_ix.pkl\"))\n",
    "\n",
    "        ## get save folder\n",
    "        savefolder = os.path.join(self.save_folder,cv_folder)\n",
    "        train_ix = cv_ix[\"train_ix\"]\n",
    "        test_ix = cv_ix[\"test_ix\"]\n",
    "\n",
    "        ## get \n",
    "        X = self.shel_data[\"X\"]\n",
    "        X = Modify_Variants_Mute(X,self.PARP_Train_X)\n",
    "\n",
    "        Y = self.shel_data[\"Results\"]\n",
    "\n",
    "        ## get train and test \n",
    "        train_x = X.to_numpy()[train_ix]\n",
    "        test_x = X.to_numpy()[test_ix]\n",
    "\n",
    "        train_y = Y.to_numpy()[train_ix]\n",
    "        test_y = Y.to_numpy()[test_ix]\n",
    "\n",
    "        ## evaluate \n",
    "        train_score = self.PARP_base_model.evaluate(train_x,train_y,verbose=0)\n",
    "        test_score = self.PARP_base_model.evaluate(test_x,test_y,verbose=0)\n",
    "\n",
    "        train_pred = self.PARP_base_model.predict(train_x,verbose=0)\n",
    "        test_pred =self.PARP_base_model.predict(test_x,verbose=0)\n",
    "\n",
    "        score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "        pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "\n",
    "        ## save \n",
    "        save_as_pickle(data=score,path = os.path.join(savefolder,\"basePARP_score.pkl\"))\n",
    "        save_as_pickle(data=pred,path = os.path.join(savefolder,\"basePARP_pred.pkl\"))\n",
    "    \n",
    "        \n",
    "    def train_and_eval_LRPT(self,cv_folder):\n",
    "        assert cv_folder in os.listdir(self.save_folder),\"Cv folder doesn't exist\"\n",
    "        cv_ix = open_pickle(os.path.join(self.save_folder,cv_folder,\"train_test_ix.pkl\"))\n",
    "\n",
    "        ## get save folder\n",
    "        savefolder = os.path.join(self.save_folder,cv_folder)\n",
    "\n",
    "        train_x = self.shel_data[\"X\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "        train_y = self.shel_data[\"Results\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "\n",
    "        test_x = self.shel_data[\"X\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "        test_y = self.shel_data[\"Results\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "\n",
    "\n",
    "        log_dir = os.path.join(savefolder,\"tensorboard/logs\")\n",
    "\n",
    "        if self.shel_config.LRPTTrain.VALSPLIT is not None:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             patience=self.shel_config.LRPTTrain.EarlyStoppingpatience, \n",
    "                                             min_delta=self.shel_config.LRPTTrain.EarlyStoppingdelta),\n",
    "                         tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "            indices = range(len(train_x))\n",
    "            train_x, val_x, train_y, val_y,train_idx,val_idx = train_test_split(train_x,train_y,indices,\n",
    "                                                                                test_size=self.shel_config.LRPTTrain.VALSIZE,\n",
    "                                                                                shuffle=self.shel_config.LRPTTrain.VALSPLITSHUFFLE,\n",
    "                                                                                random_state=self.shel_config.LRPTTrain.VALSEED)\n",
    "\n",
    "            history = self.LRPT_model.fit(x=train_x,y=train_y,\n",
    "                                          batch_size=self.shel_config.LRPTTrain.BATCH_SIZE,\n",
    "                                          epochs=self.shel_config.LRPTTrain.EPOCHS,\n",
    "                                          verbose=self.shel_config.LRPTTrain.VERBOSE,\n",
    "                                          validation_data=(val_x,val_y),\n",
    "                                          callbacks=callbacks)\n",
    "        else:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                     patience=self.shel_config.LRPTTrain.EarlyStoppingpatience, \n",
    "                                     min_delta=self.shel_config.LRPTTrain.EarlyStoppingdelta),\n",
    "                         tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "            history = self.LRPT_model.fit(x=train_x,y=train_y,\n",
    "                                          batch_size=self.shel_config.LRPTTrain.BATCH_SIZE,\n",
    "                                          epochs=self.shel_config.LRPTTrain.EPOCHS,\n",
    "                                          verbose=self.shel_config.LRPTTrain.VERBOSE,\n",
    "                                          callbacks = callbacks)\n",
    "        ## save history \n",
    "        temp_path = os.path.join(savefolder,self.shel_config.LRPTTrain.MODEL_HISTORY_PATH)\n",
    "        save_as_pickle(data =history.history,path = temp_path)\n",
    "\n",
    "        ## save model \n",
    "        h5_file = os.path.join(savefolder,self.shel_config.LRPTTrain.MODEL_WEIGHTS_PATH)\n",
    "        self.LRPT_model.save_weights(h5_file)\n",
    "\n",
    "        ## save train and val idx \n",
    "        if train_idx:\n",
    "            split_index = {\"train_ix\":train_idx,\"val_ix\":val_idx}\n",
    "            save_as_pickle(split_index,os.path.join(savefolder,\"fit_train_val_ix.pkl\"))\n",
    "\n",
    "        ## evaluate and predict \n",
    "        train_score = self.LRPT_model.evaluate(train_x,train_y,verbose=0)\n",
    "        train_pred = self.LRPT_model.predict(train_x,verbose=0)\n",
    "\n",
    "        test_score = self.LRPT_model.evaluate(test_x,test_y,verbose=0)\n",
    "        test_pred = self.LRPT_model.predict(test_x,verbose=0)\n",
    "\n",
    "        score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "        pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "\n",
    "        if train_idx:\n",
    "            val_score = self.LRPT_model.evaluate(val_x,val_y,verbose=0)\n",
    "            val_pred = self.LRPT_model.predict(val_x,verbose=0)\n",
    "            score[\"val_score\"] = val_score\n",
    "            pred[\"val_pred\"] = val_pred\n",
    "\n",
    "        save_as_pickle(data=score,path = os.path.join(savefolder,\"fit_score.pkl\"))\n",
    "        save_as_pickle(data=pred,path = os.path.join(savefolder,\"fit_pred.pkl\"))\n",
    "\n",
    "        \n",
    "        \n",
    "    def build(self,base_config_index,base_yaml_index,config_file_path):\n",
    "        base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "        parp_folder =\"/work/PCDC/s202005/Research/PARP/Results/PARPforLRPT/Single_1_Anti_2_Dense_1024\" \n",
    "        self._get_base_config(base_config_index=base_config_index,\n",
    "                              base_config_folder=base_config_folder,\n",
    "                              base_yaml_index=base_yaml_index)\n",
    "        self._get_shel_config(config_file_path=config_file_path)\n",
    "        self._get_cv_and_data()\n",
    "        self._get_base_model_path(parp_folder=parp_folder)\n",
    "        self._wrap_PARP_data()\n",
    "        self._get_base_model_PARP()\n",
    "        self._get_base_model_LRPT()\n",
    "        self._build_LRPT_model()\n",
    "    def run(self,cv_folder):\n",
    "        self.eval_on_basePARP(cv_folder=cv_folder)\n",
    "        self.train_and_eval_LRPT(cv_folder=cv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_config_index = 0 \n",
    "base_yaml_index = 0 \n",
    "config_file_path = \"/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/template.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLRPT = ShelLRPT()\n",
    "myLRPT.build(base_config_index=base_config_index,base_yaml_index=base_yaml_index,config_file_path=config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Cv folder doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_123790/327640105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyLRPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_123790/2495861366.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, cv_folder)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_basePARP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_eval_LRPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_123790/2495861366.py\u001b[0m in \u001b[0;36mtrain_and_eval_LRPT\u001b[0;34m(self, cv_folder)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_eval_LRPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mcv_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYSTEM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVEFOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Cv folder doesn't exist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mcv_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYSTEM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVEFOLDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train_test_ix.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cv folder doesn't exist"
     ]
    }
   ],
   "source": [
    "myLRPT.run(\"cv_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step0ï¼šGet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get PARP config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_config(base_config_index,base_yaml_index,base_config_folder):\n",
    "    config_folder_dic = sorted([ele for ele in os.listdir(base_config_folder) if ele.startswith(\"Epochs\")])\n",
    "    config_subfolder = os.path.join(base_config_folder,config_folder_dic[base_config_index])\n",
    "    yaml_folder = sorted([ele for ele in os.listdir(config_subfolder) if ele.endswith(\".yaml\")])\n",
    "    \n",
    "    base_yaml_path = os.path.join(config_subfolder,yaml_folder[base_yaml_index])\n",
    "    config = get_cfg_defaults()\n",
    "    config.merge_from_file(base_yaml_path)\n",
    "    return config,base_yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config_folder = \"/work/PCDC/s202005/Research/PARP/Codes/PARPforLRPT/config/Single_1_Anti_2_Dense_1024\"\n",
    "base_config_index = 0 \n",
    "base_yaml_index = 0 \n",
    "base_config,base_yaml_path = get_base_config(base_config_index,base_yaml_index,base_config_folder)\n",
    "del base_config_folder,base_config_index,base_yaml_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get Shelburn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shel_config(config_path):\n",
    "    config = get_shelburne_defaults()\n",
    "    config.merge_from_file(config_path)\n",
    "    os.makedirs(config.SYSTEM.SAVEFOLDER,exist_ok=True)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"/endosome/work/PCDC/s202005/Research/TransferLearning/Shelburne/Codes/config/template.yaml\"\n",
    "shel_config = get_shel_config(config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Get shel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_and_data(shel_config):\n",
    "    ######## get data \n",
    "    Variants,Antibiotics,Results = open_pickle(shel_config.SHELDATA.ThreeSets)\n",
    "    DataInfo = pd.read_csv(shel_config.SHELDATA.Info)\n",
    "    Shel_X = pd.concat([Variants,Antibiotics],axis=1)\n",
    "    shel_data = {\"Variants\":Variants,\n",
    "                 \"Antibiotics\":Antibiotics,\n",
    "                 \"Results\":Results,\n",
    "                 \"X\":Shel_X,\n",
    "                 \"Info\":DataInfo}\n",
    "    \n",
    "    ######### get cv \n",
    "    Totaly = Results\n",
    "    cv_split = KFold(n_splits=shel_config.LRPTTrain.CVSPLIT,\n",
    "                     shuffle=shel_config.LRPTTrain.CVSHUFFLE,\n",
    "                     random_state=shel_config.LRPTTrain.CVSEED)\n",
    "    for cv_index,(train_ix,test_ix) in enumerate(cv_split.split(Totaly)):\n",
    "        cv_save_folder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,\"cv_\"+str(cv_index))\n",
    "        os.makedirs(cv_save_folder,exist_ok=True) # make the dircitiony \n",
    "        split_index = {\"train_ix\":train_ix,\"test_ix\":test_ix}\n",
    "        save_as_pickle(split_index,os.path.join(cv_save_folder,\"train_test_ix.pkl\"))\n",
    "    return shel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shel_data = get_cv_and_data(shel_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Get base model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parp_folder =\"/work/PCDC/s202005/Research/PARP/Results/PARPforLRPT/Single_1_Anti_2_Dense_1024\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model_path(parp_folder,base_yaml_path):\n",
    "    epochsname  = os.path.split(os.path.split(base_yaml_path)[0])[1]\n",
    "    seedname = os.path.split(base_yaml_path)[1].replace(\".yaml\",\"\")\n",
    "    trained_model_weights_path = os.path.join(parp_folder,epochsname,seedname,\"modelweights.h5\")\n",
    "    return trained_model_weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model_weights_path = get_base_model_path(parp_folder,base_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build up LRPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Get PARP Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrap_PARP_data(base_config):\n",
    "    # get train/test/or validation for training process\n",
    "    Variants,Antibiotics,Results = open_pickle(base_config.PARPData.TrainPath)\n",
    "    PARP_Train_X = pd.concat([Variants,Antibiotics],1)\n",
    "    PARP_Train_X = PARP_Train_X.iloc[:5,:]  \n",
    "    return PARP_Train_X,Variants.shape[1],Antibiotics.shape[1]\n",
    "PARP_Train_X,VARIANTS_INPUT_DIM, ANTI_INPUT_DIM = wrap_PARP_data(base_config)\n",
    "opts = [\"PARP.VARIANTS_INPUT_DIM\", VARIANTS_INPUT_DIM,\n",
    "        \"PARP.ANTI_INPUT_DIM\", ANTI_INPUT_DIM]\n",
    "base_config.merge_from_list(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Get PARP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model_PARP(base_config):\n",
    "    ## step 3: get model \n",
    "    model = build_parp_model(base_config)\n",
    "    # ## step 4: model compile \n",
    "    loss_key = base_config.PARPTrain.LOSS\n",
    "    loss = [ele for ele in base_config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "    optimizer_key = base_config.PARPTrain.OPTIMIZER\n",
    "    optimizer_rate = base_config.PARPTrain.OPTIMIZERATE\n",
    "    optimizer = [ele for ele in base_config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "    metrics_threshold_list = base_config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    K.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 22:02:17.149852: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/16.05.8/lib64/slurm:/cm/shared/apps/slurm/16.05.8/lib64\n",
      "2022-03-26 22:02:17.149886: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-26 22:02:17.149911: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NucleusA149): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "base_PARP_model = get_base_model_PARP(base_config)\n",
    "base_PARP_model.load_weights(trained_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: build another PARP model with different variants dim-- LRP_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LRPT_base_config = copy.deepcopy(base_config)\n",
    "opts = [\"PARP.VARIANTS_INPUT_DIM\", shel_data[\"Variants\"].shape[1],\n",
    "        \"PARP.ANTI_INPUT_DIM\", shel_data[\"Antibiotics\"].shape[1]]\n",
    "LRPT_base_config.merge_from_list(opts)\n",
    "LRPT_base_model = build_parp_model(LRPT_base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4: change LRPT_base_model based on LRPT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRPT_model = build_LRPT_model(shel_config,LRPT_base_model,shel_data[\"X\"],\n",
    "                              base_PARP_model,PARP_Train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_LRPT_model(model,base_config):\n",
    "    # ## step 4: model compile \n",
    "    loss_key = base_config.PARPTrain.LOSS\n",
    "    loss = [ele for ele in base_config.PARPTrain.LOSSDIC if ele[0]==loss_key][0][1]\n",
    "\n",
    "    optimizer_key = base_config.PARPTrain.OPTIMIZER\n",
    "    optimizer_rate = base_config.PARPTrain.OPTIMIZERATE\n",
    "    optimizer = [ele for ele in base_config.PARPTrain.OPTIMIZERDIC if ele[0]==optimizer_key][0][1]\n",
    "\n",
    "    metrics_threshold_list = base_config.PARPTrain.METRICSTHRESHOLDLIST\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(threshold = threshold,name='binary_accuracy_'+str(threshold)) for threshold in metrics_threshold_list]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    K.set_value(model.optimizer.learning_rate, optimizer_rate)\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRPT_model = compile_LRPT_model(LRPT_model,LRPT_base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.4:Evaluate on Base PARP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_basePARP(base_PARP_model,cv_folder,base_config,shel_config,shel_data,PARP_Train_X):\n",
    "    assert cv_folder in os.listdir(shel_config.SYSTEM.SAVEFOLDER),\"Cv folder doesn't exist\"\n",
    "    cv_ix = open_pickle(os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder,\"train_test_ix.pkl\"))\n",
    "    \n",
    "    ## get save folder\n",
    "    savefolder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder)\n",
    "    train_ix = cv_ix[\"train_ix\"]\n",
    "    test_ix = cv_ix[\"test_ix\"]\n",
    "    \n",
    "    ## get \n",
    "    X = shel_data[\"X\"]\n",
    "    X = Modify_Variants_Mute(X,PARP_Train_X)\n",
    "    \n",
    "    Y = shel_data[\"Results\"]\n",
    "    \n",
    "    ## get train and test \n",
    "    train_x = X.to_numpy()[train_ix]\n",
    "    test_x = X.to_numpy()[test_ix]\n",
    "    \n",
    "    train_y = Y.to_numpy()[train_ix]\n",
    "    test_y = Y.to_numpy()[test_ix]\n",
    "    \n",
    "    ## evaluate \n",
    "    train_score = base_PARP_model.evaluate(train_x,train_y,verbose=0)\n",
    "    test_score = base_PARP_model.evaluate(test_x,test_y,verbose=0)\n",
    "    \n",
    "    train_pred = base_PARP_model.predict(train_x,verbose=0)\n",
    "    test_pred = base_PARP_model.predict(test_x,verbose=0)\n",
    "    \n",
    "    score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "    pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "    \n",
    "    ## save \n",
    "    save_as_pickle(data=score,path = os.path.join(savefolder,\"basePARP_score.pkl\"))\n",
    "    save_as_pickle(data=pred,path = os.path.join(savefolder,\"basePARP_pred.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_on_basePARP(base_PARP_model=base_PARP_model,\n",
    "                 cv_folder=\"cv_0\",base_config=base_config,\n",
    "                 shel_config=shel_config,shel_data=shel_data,\n",
    "                 PARP_Train_X = PARP_Train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5:Fit LRPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval_LRPT(LRPT_model,cv_folder,base_config,shel_config,shel_data):\n",
    "    assert cv_folder in os.listdir(shel_config.SYSTEM.SAVEFOLDER),\"Cv folder doesn't exist\"\n",
    "    cv_ix = open_pickle(os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder,\"train_test_ix.pkl\"))\n",
    "    \n",
    "    ## get save folder\n",
    "    savefolder = os.path.join(shel_config.SYSTEM.SAVEFOLDER,cv_folder)\n",
    "    \n",
    "    train_x = shel_data[\"X\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "    train_y = shel_data[\"Results\"].to_numpy()[cv_ix[\"train_ix\"]]\n",
    "    \n",
    "    test_x = shel_data[\"X\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "    test_y = shel_data[\"Results\"].to_numpy()[cv_ix[\"test_ix\"]]\n",
    "    \n",
    "    \n",
    "    log_dir = os.path.join(savefolder,\"tensorboard/logs\")\n",
    "    \n",
    "    if shel_config.LRPTTrain.VALSPLIT is not None:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=shel_config.LRPTTrain.EarlyStoppingpatience, \n",
    "                                         min_delta=shel_config.LRPTTrain.EarlyStoppingdelta),\n",
    "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "        indices = range(len(train_x))\n",
    "        train_x, val_x, train_y, val_y,train_idx,val_idx = train_test_split(train_x,train_y,indices,\n",
    "                                                                            test_size=shel_config.LRPTTrain.VALSIZE,\n",
    "                                                                            shuffle=shel_config.LRPTTrain.VALSPLITSHUFFLE,\n",
    "                                                                            random_state=shel_config.LRPTTrain.VALSEED)\n",
    "        \n",
    "        history = LRPT_model.fit(x=train_x,y=train_y,\n",
    "                                 batch_size=shel_config.LRPTTrain.BATCH_SIZE,\n",
    "                                 epochs=shel_config.LRPTTrain.EPOCHS,\n",
    "                                 verbose=shel_config.LRPTTrain.VERBOSE,\n",
    "                                 validation_data=(val_x,val_y),\n",
    "                                 callbacks=callbacks)\n",
    "    else:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                 patience=shel_config.LRPTTrain.EarlyStoppingpatience, \n",
    "                                 min_delta=shel_config.LRPTTrain.EarlyStoppingdelta),\n",
    "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "        history = LRPT_model.fit(x=train_x,y=train_y,\n",
    "                                 batch_size=shel_config.LRPTTrain.BATCH_SIZE,\n",
    "                                 epochs=shel_config.LRPTTrain.EPOCHS,\n",
    "                                 verbose=shel_config.LRPTTrain.VERBOSE,\n",
    "                                 callbacks = callbacks)\n",
    "    ## save history \n",
    "    temp_path = os.path.join(savefolder,shel_config.LRPTTrain.MODEL_HISTORY_PATH)\n",
    "    save_as_pickle(data =history.history,path = temp_path)\n",
    "    \n",
    "    ## save model \n",
    "    h5_file = os.path.join(savefolder,shel_config.LRPTTrain.MODEL_WEIGHTS_PATH)\n",
    "    LRPT_model.save_weights(h5_file)\n",
    "    \n",
    "    ## save train and val idx \n",
    "    if train_idx:\n",
    "        split_index = {\"train_ix\":train_idx,\"val_ix\":val_idx}\n",
    "        save_as_pickle(split_index,os.path.join(savefolder,\"fit_train_val_ix.pkl\"))\n",
    "    \n",
    "    ## evaluate and predict \n",
    "    train_score = LRPT_model.evaluate(train_x,train_y,verbose=0)\n",
    "    train_pred = LRPT_model.predict(train_x,verbose=0)\n",
    "    \n",
    "    test_score = LRPT_model.evaluate(test_x,test_y,verbose=0)\n",
    "    test_pred = LRPT_model.predict(test_x,verbose=0)\n",
    "    \n",
    "    score = {\"train_score\":train_score,\"test_score\":test_score}\n",
    "    pred = {\"train_pred\":train_pred,\"test_pred\":test_pred}\n",
    "    \n",
    "    if train_idx:\n",
    "        val_score = LRPT_model.evaluate(val_x,val_y,verbose=0)\n",
    "        val_pred = LRPT_model.predict(val_x,verbose=0)\n",
    "        score[\"val_score\"] = val_score\n",
    "        pred[\"val_pred\"] = val_pred\n",
    "    \n",
    "    save_as_pickle(data=score,path = os.path.join(savefolder,\"fit_score.pkl\"))\n",
    "    save_as_pickle(data=pred,path = os.path.join(savefolder,\"fit_pred.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRPT_model = train_and_eval_LRPT(LRPT_model=LRPT_model,\n",
    "                        cv_folder=\"cv_0\",\n",
    "                        base_config=base_config,\n",
    "                        shel_config=shel_config,\n",
    "                        shel_data=shel_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
